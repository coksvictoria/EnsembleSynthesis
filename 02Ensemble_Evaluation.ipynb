{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk2WlRniXyDz",
        "outputId": "5a52ed6e-75fa-4945-e5c5-f1cb9d71b688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "metadata": {
        "id": "8X0v3usN9WOl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing"
      ],
      "metadata": {
        "id": "L-YNWB2l98tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder,OneHotEncoder,MinMaxScaler\n",
        "\n",
        "def encode_df(df_raw,c_col):\n",
        "    df=df_raw.copy()\n",
        "    cat_dict={}\n",
        "    for i in c_col:\n",
        "      df[i]=df[i].astype('category')\n",
        "      cat_dict[i] = dict(enumerate(df[i].cat.categories))\n",
        "      df[i]=df[i].cat.codes\n",
        "      df[i]=df[i].astype('int')\n",
        "\n",
        "    return df,cat_dict\n",
        "\n",
        "def real_process(reald,cat_cols,y_map=None,weight=False):\n",
        "\n",
        "  real=reald.copy().reset_index(drop=True)\n",
        "  # real,_=encode_df(real,cat_cols)\n",
        "\n",
        "  X,y=real.iloc[:,:-1],real.iloc[:,-1]\n",
        "  col_names=X.columns.to_list()\n",
        "  n_col=[s for s in col_names if s not in cat_cols]\n",
        "  cat_cols = [s for s in col_names if s in cat_cols]\n",
        "\n",
        "  ss=MinMaxScaler()\n",
        "  real_scaled=pd.DataFrame(ss.fit_transform(X[n_col]), index=None,columns=n_col)\n",
        "  ohe = OneHotEncoder(sparse_output=False,handle_unknown=\"ignore\")\n",
        "\n",
        "  #One-hot-encode the categorical columns.\n",
        "  ohe_real = ohe.fit_transform(X[cat_cols])\n",
        "  #Convert it to df\n",
        "  real_encoded = pd.DataFrame(ohe_real, index=None,columns=ohe.get_feature_names_out())\n",
        "\n",
        "  if weight:\n",
        "    #One-hot-encoding with the frequency as weight\n",
        "    col_idx=0\n",
        "    for i in cat_cols:\n",
        "      n_cat=real[i].nunique()\n",
        "      real_encoded.iloc[:,col_idx:col_idx+n_cat] = real_encoded.iloc[:,col_idx:col_idx+n_cat]*1/n_cat\n",
        "      col_idx=col_idx+n_cat\n",
        "\n",
        "  if y_map is not None:\n",
        "    real_processed=pd.concat([real_scaled, real_encoded,y.map(y_map)], axis=1)\n",
        "  else:\n",
        "    real_processed=pd.concat([real_scaled, real_encoded,y], axis=1)\n",
        "\n",
        "  return real_processed,ss,ohe\n",
        "\n",
        "def fake_process(faked,cat_cols,ss,ohe,y_map=None,weight=False):\n",
        "  fake=faked.copy().reset_index(drop=True)\n",
        "  X,y=fake.iloc[:,:-1],fake.iloc[:,-1]\n",
        "\n",
        "  col_names=X.columns.to_list()\n",
        "  n_col=[s for s in col_names if s not in cat_cols]\n",
        "  cat_cols = [s for s in col_names if s in cat_cols]\n",
        "\n",
        "  fake_scaled=pd.DataFrame(ss.transform(X[n_col]), index=None,columns=n_col)\n",
        "\n",
        "  #One-hot-encode the categorical columns.\n",
        "  ohe_fake = ohe.transform(X[cat_cols])\n",
        "  #Convert it to df\n",
        "  fake_encoded = pd.DataFrame(ohe_fake, index=None,columns=ohe.get_feature_names_out())\n",
        "\n",
        "  if weight:\n",
        "    #One-hot-encoding with the frequency as weight\n",
        "    col_idx=0\n",
        "    for i in cat_cols:\n",
        "      n_cat=real[i].nunique()\n",
        "      fake_encoded.iloc[:,col_idx:col_idx+n_cat] = fake_encoded.iloc[:,col_idx:col_idx+n_cat]*1/n_cat\n",
        "      col_idx=col_idx+n_cat\n",
        "\n",
        "  if y_map is not None:\n",
        "    fake_processed=pd.concat([fake_scaled, fake_encoded,y.map(y_map)], axis=1)\n",
        "  else:\n",
        "    fake_processed=pd.concat([fake_scaled, fake_encoded,y], axis=1)\n",
        "\n",
        "  return fake_processed"
      ],
      "metadata": {
        "id": "cOjzLyGH9_Aa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define EKCD"
      ],
      "metadata": {
        "id": "A0mrA3cc9sX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np ##numpy for sorting distance\n",
        "import scipy##for distance metrics\n",
        "import numbers\n",
        "\n",
        "from imblearn.base import BaseSampler\n",
        "from sklearn.utils import _safe_indexing\n",
        "\n",
        "class EKCD(BaseSampler):\n",
        "\n",
        "    _parameter_constraints: dict = {\n",
        "        \"n_jobs\": [numbers.Integral, None]\n",
        "    }\n",
        "\n",
        "    def __init__(self,*,sampling_strategy=\"auto\", n_neighbors=15,n_vote=3,kind_sel=\"cd\",n_jobs=None,sampling_type=\"under-sampling\"):\n",
        "        super().__init__()\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.kind_sel = kind_sel\n",
        "        self.n_jobs = n_jobs\n",
        "        self._sampling_strategy=sampling_strategy\n",
        "        self.sampling_type=sampling_type\n",
        "        self._sampling_type=sampling_type\n",
        "        self.n_vote=n_vote\n",
        "\n",
        "        SAMPLING_TARGET_KIND=[\"minority\",\"majority\", \"not minority\",\"not majority\",\"all\",\"auto\"]\n",
        "\n",
        "        if isinstance(self.sampling_type, str):\n",
        "           if self._sampling_strategy not in SAMPLING_TARGET_KIND:\n",
        "            raise ValueError(\n",
        "                f\"When 'sampling_strategy' is a string, it needs\"\n",
        "                f\" to be one of {SAMPLING_TARGET_KIND}. Got '{self._sampling_strategy}' \"\n",
        "                f\"instead.\")\n",
        "\n",
        "\n",
        "    def _fit_resample(self, X, y, X_real=None, y_real=None):\n",
        "        self.X_= X\n",
        "        self.y_= y\n",
        "\n",
        "        if X_real is not None:\n",
        "          self.X_= X_real\n",
        "          self.y_=y_real\n",
        "\n",
        "        #calculate distance\n",
        "        d=scipy.spatial.distance.cdist(X,self.X_)\n",
        "        #get k lowest distance and save to Sx\n",
        "        indexes_all=np.argsort(d)[:,1:self.n_neighbors+1] # return k indexes of lowest value in d\n",
        "\n",
        "        ##check if the top n neareast names are from same group, if not then use CDNN\n",
        "        single_key = np.max(self.y_[indexes_all[:,:self.n_vote]],axis=1) == np.min(self.y_[indexes_all[:,:self.n_vote]],axis=1)\n",
        "        indexes=indexes_all[~single_key]##use KCDNN for the uncertain ones or hard ones\n",
        "\n",
        "        idx_under = np.empty((0,), dtype=int)\n",
        "        input_dim=X.shape[1]\n",
        "\n",
        "        if self.kind_sel==\"all\":\n",
        "          idx_under=np.flatnonzero(np.max(y[indexes_all],axis=1) == np.min(y[indexes_all],axis=1))\n",
        "        elif self.kind_sel==\"cd\":\n",
        "          y_pred=[] ##set y_predict list\n",
        "          for n,index in enumerate(indexes): ##looping through k indexes over the whole test dataset\n",
        "            Sx = dict()\n",
        "            for idx in range(self.n_neighbors):\n",
        "              key = index[idx]\n",
        "              if y[key] in Sx:\n",
        "                Sx[y[key]].append(X[key])\n",
        "              else:\n",
        "                Sx[y[key]] = []\n",
        "                Sx[y[key]].append(X[key])\n",
        "\n",
        "            #calculate current centroids within training dataset\n",
        "            px = dict()\n",
        "            for key in Sx:\n",
        "              sum_item = np.zeros(input_dim)\n",
        "              for i in range(len(Sx[key])):\n",
        "                sum_item += Sx[key][i]\n",
        "\n",
        "              px_item = sum_item/len(Sx[key])\n",
        "\n",
        "              px[key] = px_item\n",
        "\n",
        "            #calculate new centroid by adding new test data\n",
        "            qx = dict()\n",
        "            for key in Sx:\n",
        "              sum_item = np.zeros(input_dim)\n",
        "              for i in range(len(Sx[key])):\n",
        "                sum_item+=Sx[key][i]\n",
        "              sum_item += X[n]\n",
        "              qx_item = sum_item/(len(Sx[key]) + 1)\n",
        "              qx[key] = qx_item\n",
        "\n",
        "            #calculate displacement\n",
        "            theta = dict()\n",
        "            for key in px:\n",
        "              if key in qx:\n",
        "                theta[key] = np.linalg.norm(px[key] - qx[key])\n",
        "\n",
        "            label=min(theta, key=theta.get)\n",
        "            y_pred.append(label)\n",
        "\n",
        "          idx_under=np.flatnonzero(np.array(y_pred)==y[~single_key])\n",
        "\n",
        "        minority_class=np.argmin(np.bincount(y))\n",
        "        majority_class=np.array(np.argmax(np.bincount(y)))\n",
        "        all_class=np.unique(y)\n",
        "        non_minority=np.setdiff1d(all_class,minority_class)\n",
        "        non_majority=np.setdiff1d(all_class,majority_class)\n",
        "\n",
        "        idx_under=np.unique(np.concatenate((np.array(np.where(single_key)[0]),np.array(np.where(~single_key)[0])[idx_under])),axis=0)\n",
        "        if self._sampling_strategy in ['not minority','auto']:\n",
        "          target_class_indices= np.flatnonzero(y == int(minority_class))\n",
        "          idx_under = np.unique(np.concatenate((idx_under,target_class_indices),axis=0))\n",
        "        elif self._sampling_strategy=='not majority':\n",
        "          target_class_indices= np.flatnonzero(y == majority_class)\n",
        "          idx_under = np.unique(np.concatenate((idx_under,target_class_indices),axis=0))\n",
        "        elif self._sampling_strategy=='all':\n",
        "          pass\n",
        "        elif self._sampling_strategy=='majority':\n",
        "          for target_class in non_majority:\n",
        "            target_class_indices= np.flatnonzero(y == target_class)\n",
        "            idx_under = np.unique(np.concatenate((idx_under,target_class_indices),axis=0))\n",
        "        elif self._sampling_strategy=='minority':\n",
        "          for target_class in non_minority:\n",
        "            target_class_indices= np.flatnonzero(y == target_class)\n",
        "            idx_under = np.unique(np.concatenate((idx_under,target_class_indices),axis=0))\n",
        "\n",
        "        self.sample_indices_ = idx_under\n",
        "\n",
        "        return _safe_indexing(X, idx_under), _safe_indexing(y, idx_under)\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\"sample_indices\": True}"
      ],
      "metadata": {
        "id": "7NPQyWql9wVU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculate Density Score"
      ],
      "metadata": {
        "id": "qx352IBH-BiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import Parallel, delayed\n",
        "from typing import Union, List, Optional,Tuple, Dict, Callable,Any\n",
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "def compute_pairwise_distance(data_x: np.ndarray, data_y: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data_x: numpy.ndarray([N, feature_dim], dtype=np.float32)\n",
        "        data_y: numpy.ndarray([N, feature_dim], dtype=np.float32)\n",
        "    Returns:\n",
        "        numpy.ndarray([N, N], dtype=np.float32) of pairwise distances.\n",
        "    \"\"\"\n",
        "    if data_y is None:\n",
        "        data_y = data_x\n",
        "\n",
        "    dists = pairwise_distances(data_x, data_y)\n",
        "    return dists\n",
        "\n",
        "def get_kth_value(unsorted: np.ndarray, k: int, axis: int = -1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        unsorted: numpy.ndarray of any dimensionality.\n",
        "        k: int\n",
        "    Returns:\n",
        "        kth values along the designated axis.\n",
        "    \"\"\"\n",
        "    indices = np.argpartition(unsorted, k, axis=axis)[..., :k]\n",
        "    k_smallests = np.take_along_axis(unsorted, indices, axis=axis)\n",
        "    kth_values = k_smallests.max(axis=axis)\n",
        "    return kth_values\n",
        "\n",
        "def compute_nearest_neighbour_distances(input_features: np.ndarray, nearest_k: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_features: numpy.ndarray\n",
        "        nearest_k: int\n",
        "    Returns:\n",
        "        Distances to kth nearest neighbours.\n",
        "    \"\"\"\n",
        "    distances = compute_pairwise_distance(input_features)\n",
        "    radii = get_kth_value(distances, k=nearest_k + 1, axis=-1)\n",
        "    return radii\n",
        "\n",
        "def compute_density(real: np.ndarray, fake: np.ndarray,nearest_k: int) -> Dict:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        real: numpy.ndarray([N, feature_dim], dtype=np.float32)\n",
        "        fake: numpy.ndarray([N, feature_dim], dtype=np.float32)\n",
        "    Returns:\n",
        "        density score\n",
        "    \"\"\"\n",
        "    real_nearest_neighbour_distances = compute_nearest_neighbour_distances(real, nearest_k)\n",
        "    distance_real_fake = compute_pairwise_distance(real, fake)\n",
        "\n",
        "    density = (1.0 / float(nearest_k)) * (distance_real_fake< np.expand_dims(real_nearest_neighbour_distances, axis=1)).sum(axis=0).mean()\n",
        "    return density"
      ],
      "metadata": {
        "id": "_m5kRM3--D-x"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment"
      ],
      "metadata": {
        "id": "x_ueqyS0-GT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Syn_folder='/content/drive/MyDrive/TDS/synthetic/adult'\n",
        "GMs=['tvae.csv','ctgan.csv','copulagan.csv','tabddpm.csv','stasy.csv','tabsyn.csv']\n",
        "cat_cols=[\"workclass\",\"education\",\"education.num\",\"marital.status\",\"occupation\",\"relationship\", \"race\", \"sex\", \"native.country\"]\n",
        "y_map={' <=50K':0,' >50K':1}"
      ],
      "metadata": {
        "id": "u3FRDghR-JMy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_train=pd.read_csv(os.path.join(Syn_folder, 'real.csv'))\n",
        "real_test=pd.read_csv(os.path.join(Syn_folder, 'test.csv'))\n",
        "\n",
        "real_processed,ss,ohe=real_process(real_train,cat_cols,y_map)\n",
        "test_processed=fake_process(real_test,cat_cols,ss,ohe,y_map)\n",
        "X_test=test_processed.iloc[:,:-1]\n",
        "y_test=test_processed.iloc[:,-1]"
      ],
      "metadata": {
        "id": "cuCHHauV9W0B"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores={}\n",
        "for i in GMs:\n",
        "  print(i)\n",
        "  syn_data=pd.read_csv(os.path.join(Syn_folder, i))\n",
        "  fake_processed=fake_process(syn_data,cat_cols,ss,ohe,y_map)\n",
        "\n",
        "  d_score=compute_density(real_processed, fake_processed,5)\n",
        "  scores[i]=d_score\n",
        "\n",
        "  syn_data=fake_processed\n",
        "  Refiner=EKCD(sampling_strategy='all',n_neighbors=21,n_vote=5)\n",
        "  xx,yy=Refiner.fit_resample(syn_data.iloc[:,:-1],syn_data.iloc[:,-1].astype(int))\n",
        "  fake_filtered=pd.concat([xx,yy],axis=1)\n",
        "\n",
        "  # Export to CSV\n",
        "  fake_filtered.to_csv(os.path.join(Syn_folder,f\"{i}_filtered.csv\"), index=False)\n",
        "\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aeJ1tOtPS-I",
        "outputId": "36bf8bd2-178d-4f87-848a-54a91a3b95dc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tvae.csv': 1.18700001,\n",
              " 'ctgan.csv': 0.61200034,\n",
              " 'copulagan.csv': 0.53840001,\n",
              " 'tabddpm.csv': 1.30625001,\n",
              " 'stasy.csv': 1.06860001,\n",
              " 'tabsyn.csv': 1.53220001}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensembles = []\n",
        "ensembles_filter=[]\n",
        "# Loop through the top 3 filenames\n",
        "for i in ['tvae.csv', 'tabddpm.csv', 'tabsyn.csv']:\n",
        "    filename = os.path.join(Syn_folder,i)\n",
        "    syn_data = pd.read_csv(filename)\n",
        "    ensembles.append(syn_data)\n",
        "\n",
        "    filename = os.path.join(Syn_folder, f\"{i}_filtered.csv\")\n",
        "    syn_data = pd.read_csv(filename)\n",
        "    ensembles_filter.append(syn_data)\n",
        "\n",
        "# Combine all dataframes into one\n",
        "ensembles_df = pd.concat(ensembles, ignore_index=True)\n",
        "ensembles_filter_df = pd.concat(ensembles_filter, ignore_index=True)\n",
        "# Export the combined dataframe to 'ensemble.csv'\n",
        "ensembles_df.to_csv(os.path.join(Syn_folder, 'ensemble.csv'), index=False)\n",
        "ensembles_filter_df.to_csv(os.path.join(Syn_folder, 'ensemble_filtered.csv'), index=False)"
      ],
      "metadata": {
        "id": "kCUAN9tESsvn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run ML experiment"
      ],
      "metadata": {
        "id": "0Vb_wZaET8Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors,KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,IsolationForest,ExtraTreesClassifier,ExtraTreesRegressor\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet, LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import PCA,KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import cluster\n",
        "\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc, matthews_corrcoef,confusion_matrix, average_precision_score, roc_auc_score, accuracy_score\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score,average_precision_score,accuracy_score,precision_score,recall_score,silhouette_score,f1_score,jaccard_score,pairwise\n",
        "\n",
        "def printPerformance(labels, probs,all=True):\n",
        "  predicted_labels = np.round(probs)\n",
        "  tn, fp, fn, tp = confusion_matrix(labels, predicted_labels).ravel()\n",
        "  acc = (tp + tn) / (tn + tp + fn + fp)\n",
        "  sen = tp / (tp + fn)             # sensitivity, recall, hit rate, or true positive rate (TPR)\n",
        "  spe = tn / (tn + fp)             # specificity, selectivity or true negative rate (TNR)\n",
        "  ppv = tp / (tp + fp)             # precision or positive predictive value (PPV)\n",
        "  npv = tn / (tn + fn)\n",
        "  f1 = (2*tp) / (2*tp + fp + fn)\n",
        "  dor = (tp * tn) / (fp * fn)\n",
        "  mcc = (tp*tn - fp*fn) / math.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn))\n",
        "  roc_auc = roc_auc_score(labels, probs)\n",
        "  precision, recall, _ = precision_recall_curve(labels, probs)\n",
        "  pr_auc = auc(recall, precision)\n",
        "\n",
        "  if all:\n",
        "    result=[roc_auc,pr_auc,acc,sen,spe,ppv,npv,f1,dor,mcc]\n",
        "  return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Did9t6rcHDC",
        "outputId": "adc30457-c631-4ce0-c587-6fcace42ba4d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Syn_folder='/content/drive/MyDrive/TDS/synthetic/adult'\n",
        "GMs=['tvae.csv','ctgan.csv','copulagan.csv','tabddpm.csv','stasy.csv','tabsyn.csv']\n",
        "cat_cols=[\"workclass\",\"education\",\"education.num\",\"marital.status\",\"occupation\",\"relationship\", \"race\", \"sex\", \"native.country\"]\n",
        "y_map={' <=50K':0,' >50K':1}\n",
        "\n",
        "real_train=pd.read_csv(os.path.join(Syn_folder, 'real.csv'))\n",
        "real_test=pd.read_csv(os.path.join(Syn_folder, 'test.csv'))\n",
        "\n",
        "real_processed,ss,ohe=real_process(real_train,cat_cols,y_map)\n",
        "test_processed=fake_process(real_test,cat_cols,ss,ohe,y_map)\n",
        "X_test=test_processed.iloc[:,:-1]\n",
        "y_test=test_processed.iloc[:,-1]"
      ],
      "metadata": {
        "id": "BObvNhJCb6yy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs={}\n",
        "ensemble=pd.read_csv(os.path.join(Syn_folder, 'ensemble.csv'))\n",
        "dfs['ensemble.csv']=ensemble\n",
        "ensemble=pd.read_csv(os.path.join(Syn_folder, 'ensemble_filtered.csv'))\n",
        "dfs['ensemble_filtered.csv']=ensemble\n",
        "for i in GMs:\n",
        "  print(i)\n",
        "  syn_data=pd.read_csv(os.path.join(Syn_folder, i))\n",
        "  dfs[i]=syn_data\n",
        "  syn_data_filtered=pd.read_csv(os.path.join(Syn_folder,f\"{i}_filtered.csv\"))\n",
        "  dfs[f\"{i}_filtered\"]=syn_data_filtered"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjn64RTrcFgR",
        "outputId": "4bc40430-b34d-4c0b-d43b-dd30c750798f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tvae.csv\n",
            "ctgan.csv\n",
            "copulagan.csv\n",
            "tabddpm.csv\n",
            "stasy.csv\n",
            "tabsyn.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "estimators = [\n",
        "          KNeighborsClassifier(),\n",
        "          lgb.LGBMClassifier(random_state=1,verbose=-1 ),\n",
        "          RandomForestClassifier( random_state=1),\n",
        "          LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=500, random_state=1),\n",
        "          DecisionTreeClassifier(random_state=1),\n",
        "          MLPClassifier(solver='adam', activation='relu', learning_rate='adaptive', random_state=1)\n",
        "      ]\n",
        "estimators_names = ['KNN','LGBM','RF','LR','DT','MLP']\n",
        "\n",
        "r=[]\n",
        "\n",
        "zipped_estimators= zip(estimators_names,estimators)\n",
        "for est_name,est in zipped_estimators:\n",
        "  print(est_name)\n",
        "  print(\"-\"*50)\n",
        "  print(\"real training data\")\n",
        "  est.fit(real_processed.iloc[:,:-1],real_processed.iloc[:,-1])\n",
        "  y_proba=est.predict_proba(X_test)[:, 1]\n",
        "  r.append([est_name,'real',printPerformance(y_test, y_proba)])\n",
        "\n",
        "  for syn_name,fake in dfs.items():\n",
        "    print(syn_name)\n",
        "    print(\"-\"*30)\n",
        "    fake_processed=fake_process(fake,cat_cols,ss,ohe,y_map)\n",
        "    est.fit(fake_processed.iloc[:,:-1],fake_processed.iloc[:,-1].astype(int))\n",
        "    y_proba=est.predict_proba(X_test)[:, 1]\n",
        "    r.append([est_name,syn_name,printPerformance(y_test, y_proba)])"
      ],
      "metadata": {
        "id": "dNZUYBopUn08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Table Plot"
      ],
      "metadata": {
        "id": "nL1iOSy77B-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA,KernelPCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def table_plot(reals,fakes,dimentionality_reduction='PCA',filename=''):\n",
        "\n",
        "\n",
        "  if dimentionality_reduction=='PCA':\n",
        "    model=KernelPCA(n_components=2,kernel=\"rbf\")#, whiten=True)\n",
        "  elif dimentionality_reduction=='TSNE': ##takes too long for big dataset\n",
        "    model=TSNE(n_components = 2, perplexity=10,random_state = 1)\n",
        "\n",
        "  if reals.shape[0]>10000:\n",
        "    reals=reals.sample(10000)\n",
        "    fakes=fakes.sample(10000)\n",
        "  realt=model.fit_transform(reals)\n",
        "  faket=model.fit_transform(fakes)\n",
        "\n",
        "  print(f'real shape {realt.shape}')\n",
        "  print(f'fake shape {faket.shape}')\n",
        "  #fit the model to our data and extract the results\n",
        "  #create a dataframe from the dataset\n",
        "  real_pca = pd.DataFrame(data = realt ,columns = [\"Component 1\",\"Component 2\"])\n",
        "  fake_pca = pd.DataFrame(data = faket ,columns = [\"Component 1\",\"Component 2\"])\n",
        "\n",
        "  real_pca['dataset']='real'\n",
        "  fake_pca['dataset']='fake'\n",
        "\n",
        "  #plot the resulting data from two dimensions\n",
        "  g = sns.jointplot(data = pd.concat([real_pca,fake_pca]),\n",
        "                  x = \"Component 1\",\n",
        "                  y = \"Component 2\",\n",
        "                    palette=[\"#2171B5\",\"#6BAED6\"],\n",
        "                    joint_kws={'alpha': 0.8},\n",
        "                    hue=\"dataset\")\n",
        "  g.fig.subplots_adjust(top=0.95)  # Adjust the top margin for the title\n",
        "  g.fig.suptitle(filename, y=0.99)  # Move title above the figure\n",
        "  g.fig.set_size_inches((5, 5))\n",
        "  if filename!='':\n",
        "    g.savefig(filename+'.pdf')"
      ],
      "metadata": {
        "id": "hoBCB77I7Ew-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for syn_name,fake in dfs.items():\n",
        "  print(syn_name)\n",
        "  print(\"-\"*30)\n",
        "  fake_processed=fake_process(fake,cat_cols,ss,ohe,y_map)\n",
        "  table_plot(real_processed,fake_processed,filename=syn_name)"
      ],
      "metadata": {
        "id": "Trfa8kBg69AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optuna for LGBM"
      ],
      "metadata": {
        "id": "ddmoqT7-ejf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna --quiet\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler"
      ],
      "metadata": {
        "id": "tpPj-SRjadE2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_lgbm(trial, X, y):\n",
        "    cv_inner = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
        "    param_grid = {\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "#        \"l2_leaf_reg\": trial.suggest_int('l2_leaf_reg',0,3,1),\n",
        "        # \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 10.0),\n",
        "        # \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 10.0),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 50),\n",
        "        # \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.6, 0.8),\n",
        "        # \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 0.8),\n",
        "        # \"subsample_freq\": trial.suggest_int(\"subsample_freq\", 1, 4)\n",
        "#        \"min_child_samples\": trial.suggest_int('min_child_samples', 70, 170)\n",
        "        }\n",
        "    model = lgb.LGBMClassifier(objective=\"binary\", random_state=0, verbosity=-1, **param_grid)\n",
        "    return cross_val_score(model, X, y, cv=cv_inner, scoring='average_precision', n_jobs=-1).mean()"
      ],
      "metadata": {
        "id": "n8Yccjf0ellD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for syn_name,fake in dfs.items():\n",
        "  print(syn_name)\n",
        "  print(\"-\"*30)\n",
        "  fake_processed=fake_process(fake,cat_cols,ss,ohe,y_map)\n",
        "  X_train, y_train=fake_processed.iloc[:,:-1],fake_processed.iloc[:,-1].astype(int)\n",
        "\n",
        "  lgbm_study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=0))\n",
        "  func = lambda trial: objective_lgbm(trial, X_train, y_train)\n",
        "  lgbm_study.optimize(func, n_trials=100)\n",
        "\n",
        "  print('Fold: {} - Best trial: val-score {}, params {}'.format(i+1,\n",
        "                                                            lgbm_study.best_trial.value,\n",
        "                                                            lgbm_study.best_trial.params))\n",
        "\n",
        "  best_model = lgb.LGBMClassifier(objective=\"binary\", random_state=0,\n",
        "                                                        **lgbm_study.best_trial.params)\n",
        "\n",
        "\n",
        "  best_model.fit(X_train, y_train)\n",
        "  y_proba=best_model.predict_proba(X_test)[:, 1]\n",
        "  r.append(['Optuna',syn_name,printPerformance(y_test, y_proba)])"
      ],
      "metadata": {
        "id": "GwgNsVICidvt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}