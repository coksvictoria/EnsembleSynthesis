{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk2WlRniXyDz",
        "outputId": "7e856366-7132-4b18-f735-4b1fc848fe05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openml --quiet\n",
        "!pip install researchpy --quiet\n",
        "!pip install ucimlrepo --quiet\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo,list_available_datasets\n",
        "\n",
        "import researchpy as rp\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "import openml\n",
        "import pandas as pd\n",
        "from openml.datasets import edit_dataset, fork_dataset, get_dataset\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "byR2CL-zQ2DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_datetime(x):\n",
        "    try:\n",
        "        pd.to_datetime(x)\n",
        "    except (RuntimeError, TypeError, NameError, IOError, ValueError):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "# Define function\n",
        "def check_if_numeric(x):\n",
        "    try:\n",
        "        pd.to_numeric(x)\n",
        "    except (RuntimeError, TypeError, NameError, IOError, ValueError):\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def identify_variable_type(x):\n",
        "    # Is the column empty? If so, it will be classified as 'NA':\n",
        "    if (x.dropna().empty == True):\n",
        "        return \"NA\"\n",
        "\n",
        "    if x.dropna().nunique() > len(x)*0.99:\n",
        "        return \"u_manual\"\n",
        "\n",
        "    # Is the variable categorical? We check the number of unique values:\n",
        "    if x.dropna().nunique() == 2:\n",
        "        return \"c_binary\"\n",
        "\n",
        "    if (check_if_numeric(x) == True):\n",
        "        if ((x.dropna().shape[0] < 3000 and x.dropna().nunique()<10) or (x.dropna().nunique()<30 and x.dropna().shape[0] >= 3000)):\n",
        "          return \"c_integer\"\n",
        "    else:\n",
        "        return \"c_categorical\"\n",
        "\n",
        "    # If no numbers are present, we classify it as a string:\n",
        "    if (x.astype(str).str.contains(r\"[0-9]\").any() == False):\n",
        "        return \"c_string\"\n",
        "    # We then check if it's numeric, or predominantly numeric with some exceptions:\n",
        "    elif (check_if_numeric(x) == True):\n",
        "        if (x.dropna() % 1 == 0).all():\n",
        "            return \"n_integer\"\n",
        "        else:\n",
        "            return \"n_float\"\n",
        "    # next, we check if it's a date or a time, or predominantly datetime with some exceptions:\n",
        "    elif (check_if_datetime(x) == True):\n",
        "        return \"n_datetime\"\n",
        "    elif (x.astype(str)[x.astype(str).str.contains(r\"[0-9]\") == False].nunique() < 11 and\n",
        "          check_if_datetime(x[x.astype(str).str.contains(r\"[0-9]\") == True]) == True):\n",
        "        return \"n_datetime\"\n",
        "    # If none of the above apply, we classify the variable as string:\n",
        "    else:\n",
        "        return \"c_string\"\n",
        "\n",
        "def remove_dtname(text,list_dt):\n",
        "    for i in list_dt:\n",
        "        text = text.replace(i, '')\n",
        "    return text\n",
        "\n",
        "def schema_detect(df_raw):\n",
        "\n",
        "    df=df_raw.copy()\n",
        "    df_col_names=df.columns.to_list()\n",
        "    column_types=df.apply(identify_variable_type)\n",
        "    column_types=[s[:5] for s in column_types]\n",
        "    col_names = [i +'_'+ j for i, j in zip(list(df.columns), column_types)]\n",
        "\n",
        "    c_col=[s for s in col_names if '_c_' in s]\n",
        "    n_col=[s for s in col_names if '_n_' in s]\n",
        "    b_col=[s for s in col_names if '_bin' in s]\n",
        "    int_col=[s for s in col_names if '_int' in s]\n",
        "    manual_col=[s for s in col_names if '_man' in s]\n",
        "\n",
        "    lists=['_c_cat','_n_int','_n_flo','_c_bin','_c_str','_c_int','_u_man']\n",
        "\n",
        "    c_col=[remove_dtname(i,lists) for i in c_col]\n",
        "    n_col=[remove_dtname(i,lists) for i in n_col]\n",
        "    b_col=[remove_dtname(i,lists) for i in b_col]\n",
        "    int_col=[remove_dtname(i,lists) for i in int_col]\n",
        "    manual_col=[remove_dtname(i,lists) for i in manual_col]\n",
        "\n",
        "    df=df.drop(manual_col,axis=1)\n",
        "\n",
        "    c_col_index=[df.columns.get_loc(c) for c in c_col]\n",
        "\n",
        "    print(f'There are {len(col_names)} columns in total, {len(c_col)} categorical, {len(n_col)} numeric, {len(b_col)} binary, {len(manual_col)} manual and {len(int_col)} int.')\n",
        "\n",
        "    cat_dict={}\n",
        "\n",
        "    for i in c_col:\n",
        "      df[i]=df[i].astype('category')\n",
        "      cat_dict[i] = dict(enumerate(df[i].cat.categories))\n",
        "      df[i]=df[i].cat.codes\n",
        "      df[i]=df[i].astype('int')\n",
        "\n",
        "    df[c_col] = df[c_col].astype('object')\n",
        "\n",
        "\n",
        "\n",
        "    return [df,cat_dict,df_col_names,c_col,n_col,b_col,int_col,manual_col,c_col_index]\n",
        "\n",
        "\n",
        "def g_stats(data,c_col,n_col,dir,y_name=None):\n",
        "\n",
        "  print(f'Data directory is in {dir}')\n",
        "\n",
        "  print(f'Orginal dataset shape {data.shape}')\n",
        "\n",
        "  rp.summarize(data[n_col]).to_csv(dir+'/stats_num.csv',index=False)\n",
        "  rp.summary_cat(data[c_col]).to_csv(dir+'/stats_cat.csv',index=False)\n",
        "\n",
        "  if y_name is not None:\n",
        "    ss,rs={},{}\n",
        "    for i in n_col:\n",
        "      summary, results = rp.ttest(group1= data[i][data[y_name] == data[y_name].unique()[1]], group1_name= \"Postive\",\n",
        "                                  group2= data[i][data[y_name] == data[y_name].unique()[0]], group2_name= \"Negative\")\n",
        "      ss[i]=summary\n",
        "      rs[i]=results\n",
        "\n",
        "    pd.concat(ss).to_csv(dir+'/y_stats_num.csv')\n",
        "\n",
        "    cb={}\n",
        "    for i in c_col:\n",
        "      cb[i]=pd.concat([pd.crosstab(data[i],data[y_name]),pd.crosstab(data[i],data[y_name]).apply(lambda r: r/r.sum(), axis=0)],axis=1)\n",
        "\n",
        "    pd.concat(cb).to_csv(dir+'/y_stats_cat.csv')"
      ],
      "metadata": {
        "id": "B1OsgWEaT_L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def data_setup(data_id,data_name,DATA,repo='openml',y_name='label'):\n",
        "\n",
        "    data_folder=DATA+data_name\n",
        "    # Check whether the specified path exists or not\n",
        "    isExist = os.path.exists(data_folder)\n",
        "    if not isExist:\n",
        "      os.makedirs(data_folder)\n",
        "\n",
        "    if repo=='openml':\n",
        "      full_data= fetch_openml(data_id=data_id, as_frame=True, parser=\"pandas\")\n",
        "      X=full_data.data\n",
        "      y=full_data.target\n",
        "    elif repo=='uci':\n",
        "      # fetch dataset\n",
        "      full_data = fetch_ucirepo(id=data_id)\n",
        "\n",
        "      # data (as pandas dataframes)\n",
        "      X = full_data.data.features\n",
        "      y = full_data.data.targets\n",
        "\n",
        "    df_raw=X.copy()\n",
        "    df_raw['label']=y\n",
        "\n",
        "    pickle.dump(df_raw,open(data_folder+\"/original.pickle\",\"wb\"))\n",
        "    df,cat_dict,df_col_names,c_col,n_col,b_col,int_col,manual_col,c_col_index=schema_detect(df_raw)\n",
        "\n",
        "    g_stats(df_raw,c_col,n_col,data_folder,y_name=y_name)\n",
        "\n",
        "    c_col_x=c_col.copy()\n",
        "    try:\n",
        "      c_col_x.remove('label')\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    pickle.dump(df,open(data_folder+\"/df.pickle\",\"wb\"))\n",
        "\n",
        "    with open(data_folder+\"/metadata.pickle\", \"wb\") as pickle_out:\n",
        "      pickle.dump(c_col, pickle_out)\n",
        "      pickle.dump(n_col, pickle_out)\n",
        "      pickle.dump(b_col, pickle_out)\n",
        "      pickle.dump(c_col_x, pickle_out)\n",
        "      pickle.dump(cat_dict, pickle_out)\n",
        "      pickle.dump(c_col_index, pickle_out)\n",
        "      pickle.dump(int_col, pickle_out)\n",
        "\n",
        "\n",
        "def cv_setup(data_name,DATA,cv=5):\n",
        "\n",
        "  data_folder=DATA+data_name\n",
        "\n",
        "  df = pickle.load(open(data_folder+'/df.pickle',\"rb\"))\n",
        "  X = df.drop(['label'],axis=1)\n",
        "  y = df['label']\n",
        "\n",
        "  # Check whether the specified path exists or not\n",
        "  isExist = os.path.exists(data_folder+'/synthetic')\n",
        "  if not isExist:\n",
        "    os.makedirs(data_folder+'/synthetic')\n",
        "\n",
        "    for i in range(cv):\n",
        "      os.makedirs(data_folder+'/synthetic/seed'+str(i))\n",
        "\n",
        "      if len(np.unique(y))<10:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=i)\n",
        "      else:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
        "\n",
        "      baseline=pd.concat([X_train,y_train],axis=1)\n",
        "      df_test=pd.concat([X_test,y_test],axis=1)\n",
        "      baseline.to_csv(data_folder+'/synthetic/seed' + str(i) +'/baseline.csv',index=False)\n",
        "      df_test.to_csv(data_folder+'/synthetic/seed' + str(i) +'/df_test.csv',index=False)"
      ],
      "metadata": {
        "id": "jRFc1JYeTsyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Xy(id):\n",
        "  # This is done based on the dataset ID.\n",
        "  dataset = openml.datasets.get_dataset(id)\n",
        "\n",
        "  # Print a summary\n",
        "  print(f\"This is dataset '{dataset.name}', the target feature is \"\n",
        "      f\"'{dataset.default_target_attribute}'\")\n",
        "\n",
        "  X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute, dataset_format=\"dataframe\")\n",
        "  return X,y,categorical_indicator, attribute_names"
      ],
      "metadata": {
        "id": "aLeV8oexRAsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA='/content/drive/MyDrive/'"
      ],
      "metadata": {
        "id": "pvs1ZUHpTzKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ids=[45059,45057,43892,1114,43976,42477,44226,44053,43889,43903]\n",
        "data_names=['sick','jasmine','national-longitudinal-survey-binary','KDDCup09_upselling','eye_movements',\n",
        " 'default-of-credit-card','NewspaperChurn','compass','law-school-admission-bianry','diabetes']\n",
        "\n",
        "for data_id,data_name in zip(data_ids, data_names):\n",
        "  data_setup(data_id,data_name,DATA)\n",
        "  cv_setup(data_name,DATA,cv=5)\n",
        "\n",
        "data_ids=[144,2]\n",
        "data_names=['credit-g','adult']\n",
        "\n",
        "for data_id,data_name in zip(data_ids, data_names):\n",
        "  data_setup(data_id,data_name,DATA,repo='uci')\n",
        "  cv_setup(data_name,DATA,cv=5)"
      ],
      "metadata": {
        "id": "lFuyvWLaRDA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}